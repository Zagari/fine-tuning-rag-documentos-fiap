{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xt_iqhfiG3Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "J9lnaXRuVfPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain==0.3.0 langchain-community==0.3.0 langchain-core==0.3.0 langchain-chroma langchain-text-splitters\n"
      ],
      "metadata": {
        "id": "9Pz9ve9bF8gY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9ac4be-d513-4196-c4d6-68cabf03fc20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.57.6 requires tokenizers<=0.23.0,>=0.22.0, but you have tokenizers 0.20.3 which is incompatible.\n",
            "langchain-classic 1.0.1 requires langchain-core<2.0.0,>=1.2.5, but you have langchain-core 0.3.0 which is incompatible.\n",
            "langchain-classic 1.0.1 requires langchain-text-splitters<2.0.0,>=1.1.0, but you have langchain-text-splitters 0.3.0 which is incompatible.\n",
            "langgraph-prebuilt 1.0.7 requires langchain-core>=1.0.0, but you have langchain-core 0.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkC72xZonxDq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader, NewsURLLoader\n",
        "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "txgU-lDtUU7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r\"/content/drive/MyDrive/Colab Notebooks/fine-tuning-fiap/Aula 03 - RAG para documentos/CNN_Links.txt\") as f:\n",
        "    urls = [link.replace(\"\\n\", \"\") for link in set(f.readlines())]\n",
        "\n",
        "loader = NewsURLLoader(urls=urls)\n",
        "data = loader.load()\n",
        "print(\"Primeira notícia: \", data[0])\n",
        "print(\"\\nSegunda notícia: \", data[1])\n"
      ],
      "metadata": {
        "id": "6OdhL3p9F1Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")"
      ],
      "metadata": {
        "id": "Wk_mlCikZ_fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = '/content/drive/MyDrive/Colab Notebooks/fine-tuning-fiap/Aula 03 - RAG para documentos/db'\n",
        "\n",
        "## Here is the nmew embeddings being used\n",
        "embedding = instructor_embeddings\n",
        "\n",
        "#vectordb = Chroma.from_texts([], embedding=embedding, persist_directory=persist_directory)\n",
        "contents = [item.page_content for item in data]\n",
        "sources = [{'source_info': item.metadata['link']} for item in data]\n",
        "\n",
        "# Initialize Chroma without specifying texts initially\n",
        "vectordb = Chroma.from_texts(contents, embedding, metadatas = sources, persist_directory=persist_directory)\n",
        "\n",
        "vectordb.persist()"
      ],
      "metadata": {
        "id": "ixUbOjsmSzlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Declaração do modelo de embedding\n",
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "persist_directory = '/content/drive/MyDrive/Colab Notebooks/fine-tuning-fiap/Aula 03 - RAG para documentos/db'\n",
        "embedding = instructor_embeddings\n",
        "\n",
        "#Criação do objeto de gestão da vector db\n",
        "vectordb = Chroma(embedding_function=embedding, persist_directory=persist_directory)\n",
        "\n",
        "retriever = vectordb.as_retriever()\n",
        "query = input(\"The first question:  \")\n",
        "#Retorna os 3 primeiros itens relevantes para a pergunta do usuário\n",
        "context = retriever.get_relevant_documents(query)[:1]"
      ],
      "metadata": {
        "id": "3WSCo3B68-yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "id": "bSXgXJ0pBDLR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}